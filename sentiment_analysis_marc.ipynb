{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea222c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import neo4j\n",
    "\n",
    "from transformers import XLMRobertaTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Configuration Constants ---\n",
    "# Model\n",
    "MODEL_NAME = \"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\n",
    "\n",
    "# Neo4j\n",
    "NEO4J_BOLT_URL = \"bolt://neo4j.cych.eu:7687\"\n",
    "NEO4J_USER = \"neo4j\"\n",
    "NEO4J_PASSWORD = \"eGWdum0XyE\" # Consider using environment variables for sensitive credentials\n",
    "\n",
    "# Batching\n",
    "NEO4J_FETCH_BATCH_SIZE = 10000\n",
    "INFERENCE_BATCH_SIZE = 64\n",
    "\n",
    "# Progress Tracking\n",
    "PROGRESS_FILE = \"sentiment_progress.txt\"\n",
    "\n",
    "# --- Device Configuration ---\n",
    "def configure_device() -> torch.device:\n",
    "    \"\"\"Configures and returns the appropriate torch device (CUDA if available, else CPU).\"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    return device\n",
    "\n",
    "# --- Pre-Process function recommended by the authors ---\n",
    "def preprocess(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Preprocesses the input text by replacing mentions (@user) and links (http)\n",
    "    as recommended by the model authors.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        # Basic check for non-string types, return empty string or raise error\n",
    "        print(f\"Warning: Non-string input to preprocess: {type(text)}. Returning empty string.\")\n",
    "        return \"\"\n",
    "    new_text = []\n",
    "    for t in text.split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        new_text.append(t)\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "# --- Model, Tokenizer & Config Loading ---\n",
    "def load_sentiment_model_and_tokenizer(model_name: str, device: torch.device):\n",
    "    \"\"\"\n",
    "    Loads the XLM-RoBERTa tokenizer, AutoModelForSequenceClassification, and AutoConfig.\n",
    "\n",
    "    model_name (str): The name of the pre-trained model.\n",
    "    device (torch.device): The device (CPU/CUDA) to load the model onto.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing (tokenizer, model, config).\n",
    "    \"\"\"\n",
    "    print(f\"\\nLoading model, tokenizer, and config for: {model_name}\")\n",
    "    try:\n",
    "        tokenizer = XLMRobertaTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "        print(\"Loaded fast XLMRobertaTokenizer.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load fast XLMRobertaTokenizer: {e}\")\n",
    "        print(\"Falling back to slow tokenizer.\")\n",
    "        tokenizer = XLMRobertaTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "        print(\"Loaded slow XLMRobertaTokenizer.\")\n",
    "\n",
    "    config = AutoConfig.from_pretrained(model_name) # To get label mapping (id2label)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "    model.to(device) # Move model to GPU/CPU\n",
    "    model.eval()\n",
    "    print(\"Model loaded successfully.\")\n",
    "    return tokenizer, model, config\n",
    "\n",
    "# --- Neo4j Bolt Setup ---\n",
    "def connect_to_neo4j(bolt_url: str, user: str, password: str) -> neo4j.GraphDatabase.driver:\n",
    "    \"\"\"\n",
    "    Establishes and verifies a connection to the Neo4j Bolt database.\n",
    "\n",
    "    bolt_url (str): The URL of the Neo4j Bolt server.\n",
    "    user (str): The Neo4j username.\n",
    "    password (str): The Neo4j password.\n",
    "\n",
    "    Returns:\n",
    "        neo4j.GraphDatabase.Driver: The Neo4j driver object.\n",
    "\n",
    "    Raises:\n",
    "        SystemExit: If connection fails.\n",
    "    \"\"\"\n",
    "    print(f\"\\nAttempting to connect to Neo4j Bolt at {bolt_url}...\")\n",
    "    try:\n",
    "        driver = neo4j.GraphDatabase.driver(bolt_url, auth=(user, password))\n",
    "        driver.verify_connectivity()\n",
    "        print(\"Successfully connected to Neo4j Bolt.\")\n",
    "        return driver\n",
    "    except Exception as e:\n",
    "        print(f\"Error connecting to Neo4j Bolt at {bolt_url}: {e}\")\n",
    "        sys.exit(\"Exiting due to Neo4j connection error.\")\n",
    "\n",
    "# --- Resume Functionality & Progress File ---\n",
    "def load_progress(filename: str) -> int:\n",
    "    \"\"\"Loads the last saved skip value from a file.\"\"\"\n",
    "    if os.path.exists(filename):\n",
    "        try:\n",
    "            with open(filename, \"r\") as f:\n",
    "                return int(f.read().strip())\n",
    "        except (ValueError, IOError) as e:\n",
    "            print(f\"Warning: Could not read progress file {filename}. Starting from 0. Error: {e}\")\n",
    "            return 0\n",
    "    return 0 # Start from 0 if the file doesn't exist\n",
    "\n",
    "def save_progress(filename: str, skip_value: int):\n",
    "    \"\"\"Saves the current skip value to a file.\"\"\"\n",
    "    try:\n",
    "        with open(filename, \"w\") as f:\n",
    "            f.write(str(skip_value))\n",
    "    except IOError as e:\n",
    "        print(f\"Error: Could not save progress to file {filename}. Error: {e}\")\n",
    "\n",
    "# --- Neo4j Transaction Functions (passed to driver.session.execute_...) ---\n",
    "def get_total_unprocessed_tweets_tx_fn(tx: neo4j.Transaction) -> int:\n",
    "    \"\"\"Transaction function to get the total count of tweets to process.\"\"\"\n",
    "    query = \"\"\"\n",
    "        MATCH (t:Tweet)\n",
    "        WHERE t.sentiment_label IS NULL\n",
    "        RETURN count(t) AS total_to_process\n",
    "    \"\"\"\n",
    "    result = tx.run(query)\n",
    "    record = result.single()\n",
    "    return record[\"total_to_process\"] if record else 0\n",
    "\n",
    "def fetch_tweets_batch_tx_fn(tx: neo4j.Transaction, skip: int, limit: int) -> list[dict]:\n",
    "    \"\"\"Transaction function to fetch a batch of tweets.\"\"\"\n",
    "    query = f\"\"\"\n",
    "        MATCH (t:Tweet)\n",
    "        WHERE t.sentiment_label IS NULL\n",
    "        RETURN id(t) AS node_id, t.text AS text, t.id AS tweet_id\n",
    "        SKIP {skip} LIMIT {limit}\n",
    "    \"\"\"\n",
    "    result = tx.run(query)\n",
    "    return [record.data() for record in result]\n",
    "\n",
    "def update_tweets_batch_tx_fn(tx: neo4j.Transaction, batch_data: list[dict]):\n",
    "    \"\"\"Transaction function to update a batch of tweets with sentiment data.\"\"\"\n",
    "    query = \"\"\"\n",
    "        UNWIND $batch_data AS params\n",
    "        MATCH (t:Tweet) WHERE id(t) = params.nodeId\n",
    "        SET t.positive = params.pos,\n",
    "            t.neutral = params.neu,\n",
    "            t.negative = params.negative,\n",
    "            t.sentiment_score = params.sentiment_score,\n",
    "            t.sentiment_label = params.sentiment_label\n",
    "    \"\"\"\n",
    "    tx.run(query, batch_data=batch_data)\n",
    "\n",
    "# --- Main Logic Functions ---\n",
    "def get_total_unprocessed_tweets_count(driver: neo4j.GraphDatabase.driver) -> int:\n",
    "    \"\"\"Fetches the total count of tweets that still need sentiment analysis.\"\"\"\n",
    "    total_tweets_to_process = 0\n",
    "    try:\n",
    "        print(\"\\nFetching total count of tweets to process via Bolt...\")\n",
    "        with driver.session() as session:\n",
    "            total_tweets_to_process = session.execute_read(get_total_unprocessed_tweets_tx_fn)\n",
    "        print(f\"Total tweets matching criteria: {total_tweets_to_process}\")\n",
    "        return total_tweets_to_process\n",
    "    except neo4j.exceptions.Neo4jError as e:\n",
    "        print(f\"Error fetching total count from Neo4j: {e}\")\n",
    "        sys.exit(\"Cannot show overall progress without total count. Exiting.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during total count fetch: {e}\")\n",
    "        sys.exit(\"Cannot show overall progress due to unexpected error. Exiting.\")\n",
    "\n",
    "def fetch_tweets_batch(driver: neo4j.GraphDatabase.driver, skip: int, limit: int) -> list[dict] | None:\n",
    "    \"\"\"Fetches a batch of tweets from Neo4j.\"\"\"\n",
    "    fetched_tweets_data = []\n",
    "    try:\n",
    "        print(f\"\\nFetching batch starting at skip {skip} (limit {limit}) via Bolt...\")\n",
    "        with driver.session() as session:\n",
    "            fetched_tweets_data = session.execute_read(\n",
    "                fetch_tweets_batch_tx_fn,\n",
    "                skip,\n",
    "                limit\n",
    "            )\n",
    "        return fetched_tweets_data\n",
    "    except neo4j.exceptions.Neo4jError as e:\n",
    "        print(f\"Error fetching data from Neo4j for batch starting at {skip}: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during Neo4j fetch: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_batch_for_sentiment(\n",
    "    fetched_tweets_data: list[dict],\n",
    "    tokenizer: XLMRobertaTokenizer,\n",
    "    model: AutoModelForSequenceClassification,\n",
    "    config: AutoConfig,\n",
    "    device: torch.device,\n",
    "    inference_batch_size: int\n",
    ") -> list[dict]:\n",
    "    \"\"\"\n",
    "    Processes a batch of fetched tweets for sentiment analysis.\n",
    "\n",
    "    fetched_tweets_data (list[dict]): List of dictionaries, each containing 'node_id', 'text', 'tweet_id'.\n",
    "    tokenizer, model, config: HuggingFace objects for sentiment analysis.\n",
    "    device (torch.device): The device (CPU/CUDA) to run inference on.\n",
    "    inference_batch_size (int): Batch size for model inference.\n",
    "\n",
    "    Returns:\n",
    "        list[dict]: A list of dictionaries, each containing sentiment data for a tweet,\n",
    "                    ready for Neo4j update.\n",
    "    \"\"\"\n",
    "    node_ids_batch = [tweet[\"node_id\"] for tweet in fetched_tweets_data]\n",
    "    tweet_ids_batch = [tweet[\"tweet_id\"] for tweet in fetched_tweets_data]\n",
    "    texts_batch = [preprocess(tweet[\"text\"]) for tweet in fetched_tweets_data]\n",
    "\n",
    "    all_update_params = []\n",
    "    total_texts = len(texts_batch)\n",
    "\n",
    "    # Process in smaller inference batches\n",
    "    for i in tqdm(range(0, total_texts, inference_batch_size), desc=f\"Inferring {total_texts} tweets\"):\n",
    "        sub_batch_texts = texts_batch[i : i + inference_batch_size]\n",
    "        sub_batch_node_ids = node_ids_batch[i : i + inference_batch_size]\n",
    "        sub_batch_tweet_ids = tweet_ids_batch[i : i + inference_batch_size]\n",
    "\n",
    "        if not sub_batch_texts:\n",
    "            continue\n",
    "\n",
    "        # Tokenize batch\n",
    "        inputs = tokenizer(\n",
    "            sub_batch_texts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=256\n",
    "        ).to(device)\n",
    "\n",
    "        # Perform model inference on batch\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        # Get probabilities and calculate sentiment scores/labels\n",
    "        probabilities = torch.softmax(outputs.logits, dim=1).cpu().numpy()\n",
    "\n",
    "        for j in range(probabilities.shape[0]):\n",
    "            scores = probabilities[j]\n",
    "            neg, neu, pos = float(scores[0]), float(scores[1]), float(scores[2])\n",
    "\n",
    "            sentiment_score_val = round(pos - neg, 4)\n",
    "            predicted_class_id = np.argmax(scores)\n",
    "            sentiment_label_val = config.id2label[predicted_class_id]\n",
    "\n",
    "            all_update_params.append({\n",
    "                \"nodeId\": sub_batch_node_ids[j],\n",
    "                \"pos\": pos,\n",
    "                \"neu\": neu,\n",
    "                \"negative\": neg,\n",
    "                \"sentiment_score\": sentiment_score_val,\n",
    "                \"sentiment_label\": sentiment_label_val,\n",
    "                \"tweet_id\": sub_batch_tweet_ids[j]\n",
    "            })\n",
    "    return all_update_params\n",
    "\n",
    "def update_neo4j_with_sentiment(driver: neo4j.GraphDatabase.driver, update_params: list[dict], current_skip: int) -> bool:\n",
    "    \"\"\"\n",
    "    Updates a batch of tweets in Neo4j with calculated sentiment data.\n",
    "\n",
    "    driver (neo4j.GraphDatabase.Driver): The Neo4j driver object.\n",
    "    update_params (list[dict]): List of dictionaries with sentiment data for updates.\n",
    "    current_skip (int): The starting skip value for the current batch (for logging).\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the update was successful, False otherwise.\n",
    "    \"\"\"\n",
    "    if not update_params:\n",
    "        print(\"No sentiment data to update for this batch.\")\n",
    "        return True # Consider it successful if nothing needed updating\n",
    "\n",
    "    try:\n",
    "        print(f\"\\nAttempting batch update of {len(update_params)} tweets via Bolt...\")\n",
    "        with driver.session() as session:\n",
    "            session.execute_write(\n",
    "                update_tweets_batch_tx_fn,\n",
    "                batch_data=update_params\n",
    "            )\n",
    "        print(f\"Successfully updated {len(update_params)} tweets in Neo4j for batch starting at {current_skip}.\")\n",
    "        return True\n",
    "    except neo4j.exceptions.Neo4jError as e:\n",
    "        print(f\"Error updating Neo4j for batch starting at {current_skip}: {e}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during Neo4j update: {e}\")\n",
    "        return False\n",
    "\n",
    "# --- Main Execution Function ---\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to orchestrate the sentiment analysis process for Neo4j tweets.\n",
    "    \"\"\"\n",
    "    # 1. Device Configuration\n",
    "    device = configure_device()\n",
    "\n",
    "    # 2. Load Model, Tokenizer & Config\n",
    "    tokenizer, model, config = load_sentiment_model_and_tokenizer(MODEL_NAME, device)\n",
    "\n",
    "    # 3. Connect to Neo4j\n",
    "    driver = connect_to_neo4j(NEO4J_BOLT_URL, NEO4J_USER, NEO4J_PASSWORD)\n",
    "\n",
    "    # 4. Get Total Count of Tweets to Process\n",
    "    total_tweets_to_process = get_total_unprocessed_tweets_count(driver)\n",
    "\n",
    "    # 5. Load/Adjust Progress\n",
    "    skip = load_progress(PROGRESS_FILE)\n",
    "    print(f\"Starting processing from skip: {skip}\")\n",
    "\n",
    "    # Adjust skip if the total count changed or points beyond current total\n",
    "    if total_tweets_to_process > 0 and skip > total_tweets_to_process:\n",
    "        print(f\"Warning: Loaded skip value ({skip}) is greater than current total ({total_tweets_to_process}). Resetting skip to total.\")\n",
    "        skip = total_tweets_to_process\n",
    "        save_progress(PROGRESS_FILE, skip)\n",
    "    elif total_tweets_to_process == 0:\n",
    "        print(\"No tweets found needing sentiment analysis. Exiting.\")\n",
    "        driver.close()\n",
    "        return\n",
    "\n",
    "    # 6. Main Processing Loop\n",
    "    while True:\n",
    "        # Fetch a batch of tweets from Neo4j\n",
    "        fetched_tweets_data = fetch_tweets_batch(driver, skip, NEO4J_FETCH_BATCH_SIZE)\n",
    "\n",
    "        if fetched_tweets_data is None: # An error occurred during fetch\n",
    "            print(\"Stopping due to Neo4j fetch error.\")\n",
    "            break\n",
    "        if not fetched_tweets_data:\n",
    "            print(f\"No more tweets matching criteria found in batch starting at skip {skip}. Assuming processing is complete.\")\n",
    "            break\n",
    "\n",
    "        print(f\"Fetched {len(fetched_tweets_data)} tweets from Neo4j (skip={skip}).\")\n",
    "\n",
    "        # Process the fetched batch for sentiment\n",
    "        all_update_params = process_batch_for_sentiment(\n",
    "            fetched_tweets_data, tokenizer, model, config, device, INFERENCE_BATCH_SIZE\n",
    "        )\n",
    "\n",
    "        # Update Neo4j with sentiment data\n",
    "        update_successful = update_neo4j_with_sentiment(driver, all_update_params, skip)\n",
    "\n",
    "        if update_successful:\n",
    "            # Save progress AFTER successful update\n",
    "            next_skip = skip + NEO4J_FETCH_BATCH_SIZE\n",
    "            save_progress(PROGRESS_FILE, next_skip)\n",
    "\n",
    "            # Report current progress\n",
    "            processed_count = min(next_skip, total_tweets_to_process)\n",
    "            print(f\"Progress: {processed_count} / {total_tweets_to_process} tweets processed.\")\n",
    "\n",
    "            # Prepare for next iteration\n",
    "            skip = next_skip\n",
    "        else:\n",
    "            print(\"Stopping due to Neo4j update error.\")\n",
    "            break # Break the main loop on update errors\n",
    "\n",
    "    # 7. Close the Neo4j Driver\n",
    "    if driver:\n",
    "        print(\"\\nClosing Neo4j driver connection.\")\n",
    "        driver.close()\n",
    "\n",
    "    # Final message\n",
    "    print(\"\\nFinished processing all tweets.\")\n",
    "    final_processed = load_progress(PROGRESS_FILE)\n",
    "    reported_processed = min(final_processed, total_tweets_to_process)\n",
    "    print(f\"Estimated processed based on last saved skip: {reported_processed} / {total_tweets_to_process}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dbl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
